"""A parser and lexer which use Bison-generated parse tables.

You can use the bisonx.py script to extract the tables in the required
format from the .tab.c file generated from a .y grammar file by bison.
This script requires that you strip all C-language statements from your .y
file; only the %start, %token, %%, and the grammar rules themselves may
appear.

Example
-------
A BisonParser instance requires a BisonLexer instance to tokenize the
input stream, that is, to provide the terminals of the grammar.  Here
is how to parse an input stream::

   parser = BisonParser(tables, rules, special_rule)
   parser.parse(BisonLexer(regexp, maxtok, handler).on(stream))

The BisonLexer .on() method returns an iterator yielding the sequence
of tokens in the stream, which is what the BisonParser .parse() method
needs to do its job.  The rules and special_rules are methods of some
kind of parse tree object, which is built as a side effect of the
parse call.  The handler function converts raw tokens generated by
regexp.finditer() into the token numbers required by the Bison parser
tables.

"""


class BisonParser(object):
    """Re-entrant parser built from Bison-generated parse tables.

    Also requires regexp and handler function that produce the terminals
    of the grammar from an input stream or line of text.
    """
    def __init__(self, regexp, handler, tables, rules, special_rule=None):
        """Create re-entrant parser from Bison-generated parse tables.

        Parameters
        ----------
        regexp : re
            A regular expression which splits the line into tokens, that
            is, regexp.finditer(line) should generate the tokens on line.
        handler : function
            Callback function returning the token number and value
            associated with the Match generated by the regexp::

                itoken, value = handler(match, prev_match)

            The prev_match is None if this is the first token on the
            line, otherwise the match from the previous token, so handler
            may use prev_match.end(), for example.
        tables : dict
            A dict containing the tables generated by the bisonx.py script.
        rules : list of functions
            A list of actions performed when the parser reduces the input
            according to a rule.  That is, when the parser reduces rule i,
            it will call::

                rules[i](stack)

            where stack is the parser stack; items -nrhs to -1 on the top
            of the stack are the (state, value) pairs corresponding to the
            symbols on the right hand side of rule i.
        special_rule : function, optional
            A rule to handle tokens outside the formal grammar, like
            comment or directives.  BisonLexer assumes the first such
            token has itoken larger than the maximum value for a token
            that the tables can accept, and that the out-of-band state
            continues until the next end-of-line.  (The handler can
            detect the beginning of a new line using prev_match.)
            The calling sequence for special_rule differe from rules[i]::

                special_rule(itoken, value)
        """
        r1, pgoto = tables["r1"], tables["pgoto"]
        ntokens = max(r1) - len(pgoto) + 1  # terminals of the grammar
        self.lexer = BisonLexer(regexp, ntokens, handler)
        if len(rules) != len(r1):
            raise ValueError("rule table size does not match bison tables")
        self.tables = tables
        self.rules = rules
        # The special rule is an unlisted rule to deal with comments
        # or lexical tie-ins outside of the formal grammar.
        self.special_rule = special_rule
        ntokens = max(r1) - len(pgoto) + 1  # terminals of the grammar
        self.parse(Ellipsis)

    def parse(self, stream=None):
        if isinstance(stream, str) or stream is None:
            # parse a single line (or None to generate EOF)
            for itok in self.lexer.tokens(stream):
                self.nerrs = self.automaton.send(token)
            return self.nerrs
        # reset the parser
        self.lexer.lineno = 0
        self.nerrs = 0
        self.automaton = self._automaton()
        next(self.iterator)  # advance to first yield for next_token()
        if stream is Ellipsis:
            return 0
        # otherwise, do complete parse of entire input stream
        self.lexer.on(stream)
        for nerrs in self.automaton.send(token):
            pass
        return nerrs

    def _automaton(self):
        pact, defact, table, check, pgoto, defgoto, r1, r2, final = [
            self.tables[nm] for nm in ["pact", "defact", "table", "check",
                                       "pgoto", "defgoto", "r1", "r2", "final"]]
        parse_rules = self.rules
        special_rule = self.special_rule
        lexer = self.lexer
        last = len(table) - 1
        default_pact = min(pact)  # take default action
        ntokens = max(r1) - len(pgoto) + 1  # terminals of the grammar
        # tokens have EOF, error, UNDEF prepended as 0, 1, 2
        eof_token, err_token, undef_token = 0, 1, 2

        state = 0  # 0 is initial state
        nerrs = errstatus = 0
        stack = [(state, None)]
        lookahead = None
        aborted = False

        while state != final:
            i = pact[state]
            if i == default_pact:
                rule = defact[state]
            else:
                if lookahead is None:
                    # At this point, need new token to progress.
                    lookahead = yield nerrs
                    while lexer.special:
                        special_rule(lookahead, lexer)
                        lookahead = yield nerrs
                i += lookahead
                if i >= 0 and i <= last and check[i] == lookahead:
                    j = table[i]  # >0 state for shift, <0 -rule for reduce
                    if j > 0:
                        state = j  # table entry is new state
                        if errorstatus:
                            errorstatus -= 1
                        # ---------- shift lookahead token ------------
                        stack.append((state, lexer.value))
                        lookahead = None
                        continue
                    rule = -j  # table entry is minus reduction rule
            if rule:
                # ----------- reduce according to rule --------------
                reducer = parse_rules[rule]
                nargs = r2[rule]
                args = reducer.args
                value = reducer(*[stack[i][1] for i in args])
                del stack[-nargs:]
                state, _ = stack[-1]
                # non-default gotos for this reduction also stored in table
                lhs = r1[rule] - ntokens  # non-terminal number
                i = state + pgoto[lhs]
                if i >= 0 and i <= last and check[i] == state:
                    state = table[state]
                else:
                    state = defgoto[lhs]
                stack.append((state, value))
            else:
                # ----------- syntax error -------------
                if not errstatus:
                    nerrs += 1
                elif errstatus == 3:
                    if lookahead == eof_token:
                        break  # abort if at EOF
                    lookahead = None  # failed to reuse lookahead token
                errstatus = 3  # resume parse after shifting 3 tokens
                j = 0
                while j <= 0:
                    i = pact[state]
                    if i != default_pact:
                        i += err_token
                        if i >= 0 and i <= last and check[i] == err_token:
                            j = table[i]
                            continue
                    if not stack:
                        break
                    state, _ = stack.pop()  # pop until state handles error
                else:
                    # found state that shifts err_token, push it
                    state = j
                    stack.append((state, None))  # value None okay??
                    continue
                aborted = True
                break  # no state shifts err_token, abort

        if aborted:
            # unhandled error token
            pass
        self.parse(Ellipsis)  # automatic reset when parse finishes


class BisonLexer(object):
    def __init__(self, regexp, ntokens, handler):
        self.regexp = regexp
        self.ntokens = ntokens
        self.handler = handler
        self.lineno = 0

    def on(stream):
        self._next_token = self.tokenize(
            stream, self.regexp, self.ntokens, self.handler)
        return self

    def tokenize(self, stream, regexp, ntokens, handler):
        self.lineno = 0
        for line in stream:
            for itok in self.tokens(line):
                if not itok:
                    break
                yield itok
            else:
                continue
            break
        else:
            self.m = self.value = None
        self.special = False
        yield 0

    def tokens(line=None):
        self.special = False
        self.value = None
        self.m = mprev = None
        if line is None:
            yield 0
        else:
            self.lineno += 1
            for m in regexp.finditer(line):
                # mprev provided so that handler knows mprev.end()
                # Note that mprev == None means this is new line.
                # Tokens greater than ntokens-1 reserved for conditions
                # outside formal grammar handled by special_rule() method,
                # like comments or special items.  Here, we assume all
                # of these extend to end-of-line.
                itok, value = handler(m, mprev)
                if itok >= ntokens:
                    self.special = True  # remains True until end-of-line
                self.value = value
                self.m = mprev = m
                if not itok:
                    break
                yield itok

    def __iter__(self):
        return self

    def __next__(self):
        # As side effect, sets:
        # self.value = string for SYMBOL, PRIMTYPE, or SPECIAL
        #            = number for INTEGER or PLUSSES/MINUSES (count)
        # self.special = True if this token on line beginning with SPECIAL
        # self.lineno = line number this token is on
        # self.m = match object for this token, for error messages
        #              m.string is line, m.start(), m.end() positions on line
        #              also m.group() may be useful for comments
        return next(self._next_token)

    def __call__(self):
        return next(self._next_token)
